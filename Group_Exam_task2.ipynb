{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23010718",
   "metadata": {},
   "source": [
    "Machine learning INF284 Group Exam: Markus, Nikita, Bo og Andre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806075fe",
   "metadata": {},
   "source": [
    "# Task 1 Machine learning on tabular mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd336a9",
   "metadata": {},
   "source": [
    "# Task 2 Sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21645f0d",
   "metadata": {},
   "source": [
    "Looking at this task, we noticed it had a resemblance to the sentiment analysis task like in the third lab exercise. We took this into account and tried using the same methods as in the exercise, and seeing what we could change, do differently, or do better. The two models shown in the lab are Bernoulli Naive Bayes and Multinomial Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e249d2d",
   "metadata": {},
   "source": [
    "Why would we use Bernoulli and Multinomial Naive Bayes?\n",
    "The Bernoulli Naive Bayes model looks at the true/false value if a certain word exists, to decide which kind of sentiment it has based on what it has learned. \n",
    "The Multinomial Naive Bayes model looks at the word count of multiple words and then decides the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c916a4",
   "metadata": {},
   "source": [
    "We first have to start with the initialization of data frame, containing both the text and sentiment connected to it, and this applies for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2f2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sent_id                                               text     label\n",
      "0  201911-01-01                                      Philips 190G6   Neutral\n",
      "1  201911-02-01  Med integrerte høyttalere som på ingen måte er...   Neutral\n",
      "2  201911-02-02                             Eller bedrar skinnet ?  Negative\n",
      "3  201911-03-01  De fleste skjermer har et diskret design , med...   Neutral\n",
      "4  201911-03-02  Men 190G6 fra Philips er en helt annen historie .   Neutral\n"
     ]
    }
   ],
   "source": [
    "# opening the train and test files\n",
    "f_train = open(\"task_2/3class/train.json\", encoding=\"utf-8\")\n",
    "f_test = open(\"task_2/3class/test.json\", encoding=\"utf-8\")\n",
    "\n",
    "import json\n",
    "import pandas\n",
    "\n",
    "data_train = json.load(f_train)\n",
    "data_test = json.load(f_test)\n",
    "\n",
    "df_train = pandas.DataFrame(data_train)\n",
    "df_test = pandas.DataFrame(data_train)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1995d",
   "metadata": {},
   "source": [
    "We can remove the sent_id column in both data frames, as it is redundant because the model only requires the text and sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47efe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0                                      Philips 190G6   Neutral\n",
      "1  Med integrerte høyttalere som på ingen måte er...   Neutral\n",
      "2                             Eller bedrar skinnet ?  Negative\n",
      "3  De fleste skjermer har et diskret design , med...   Neutral\n",
      "4  Men 190G6 fra Philips er en helt annen historie .   Neutral\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop('sent_id', axis=1)\n",
    "df_test = df_test.drop('sent_id', axis=1)\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22741fd",
   "metadata": {},
   "source": [
    "If we look at the text above in the first five rows, we can see that there are things we can remove, like characters and numbers.\n",
    "We should also make all the characters lowercase so that there isn't a difference between \"Test\" and \"test\".\n",
    "We create a function to handle a line or string of text and clean it of non-necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8e7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(string):\n",
    "    # removing these characters from the string\n",
    "    toRemove = [\":\", \",\", \".\", '\"', \"-\", \"/\", \"?\", \"«\", \"(\", '»', \")\",\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "    pattern = '[' + ''.join(toRemove) + ']'\n",
    "    # Remove characters matched by pattern\n",
    "    string = re.sub(pattern, '', string)\n",
    "    \n",
    "    listOfWords = string.split(\" \") # making the string into a list of words\n",
    "    listOfWords = [word.lower() for word in listOfWords] # making words lowercase\n",
    "    listOfWords = [word for word in listOfWords if word != \"\"] # removing empty characters in the list    \n",
    "    return \" \".join(listOfWords) #returning string from the list of words joined by one space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631fd82",
   "metadata": {},
   "source": [
    "Using this function on each text in our data frame to help remove the text of unnecessary characters. Lets look at a couple of  lines as an example to show the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0adb2a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philips 190G6\n",
      "De fleste skjermer har et diskret design , med smale rammer og slank fot .\n",
      "LES OGSÅ :\n",
      "I hvert fall når man ikke er alene hjemme ...\n",
      "Kan vennskapet fastholdes ?\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"][0])\n",
    "print(df_train[\"text\"][3])\n",
    "print(df_train[\"text\"][6])\n",
    "print(df_train[\"text\"][37])\n",
    "print(df_train[\"text\"][123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3e7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text\"] = df_train[\"text\"].apply(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d02a0a",
   "metadata": {},
   "source": [
    "Let's look at the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab765c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philips g\n",
      "de fleste skjermer har et diskret design med smale rammer og slank fot\n",
      "les også\n",
      "i hvert fall når man ikke er alene hjemme\n",
      "kan vennskapet fastholdes\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"][0])\n",
    "print(df_train[\"text\"][3])\n",
    "print(df_train[\"text\"][6])\n",
    "print(df_train[\"text\"][37])\n",
    "print(df_train[\"text\"][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add31aae",
   "metadata": {},
   "source": [
    "One more step we can do is to lemmatize the words, which means taking the words and putting them into their ground forms. This will make it much simpler for the algorithm to see similarities and differences in the sentences, making this sentiment analysis task much easier with higher word counts for positive, neutral, and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3f4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import NorwegianStemmer\n",
    "lemmatizer = NorwegianStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ed648",
   "metadata": {},
   "source": [
    "Let's lemmatize each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27127951",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df_train[\"text\"])):\n",
    "    line = df_train[\"text\"][index]\n",
    "    words = line.split(\" \")\n",
    "    lemmatized = []\n",
    "    for word in words:\n",
    "        lemmatized.append(lemmatizer.stem(word))\n",
    "    line = \" \".join(lemmatized)\n",
    "    df_train[\"text\"][index] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0f6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philip g\n",
      "de flest skjerm har et diskr design med smal ramm og slank fot\n",
      "les også\n",
      "i hvert fall når man ikk er alen hjemm\n",
      "kan vennskap fasthold\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"][0])\n",
    "print(df_train[\"text\"][3])\n",
    "print(df_train[\"text\"][6])\n",
    "print(df_train[\"text\"][37])\n",
    "print(df_train[\"text\"][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c1f39",
   "metadata": {},
   "source": [
    "Well we can see that there are still some unnecessary words that we can remove to help the accuracy of the model. This would be for example words like \"i\", \"er\", etc. To help us remove stop words, we used nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bbf1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "no_stopwords = stopwords.words(\"norwegian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d4f98",
   "metadata": {},
   "source": [
    "Let's go through the dataframe to remove all these unnecessary words.\n",
    "We have to go through and retrieve each string, split up the words, and then check if each word is in this stopwords list, and if so, remove it. Again let's use the same examples as before to see the difference it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5214668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df_train[\"text\"])):\n",
    "    line = df_train[\"text\"][index]\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word in no_stopwords:\n",
    "            words.remove(word)\n",
    "    line = \" \".join(words)\n",
    "    df_train[\"text\"][index] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63cb181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philip g\n",
      "flest skjerm et diskr design smal ramm slank fot\n",
      "les\n",
      "hvert fall man ikk alen hjemm\n",
      "vennskap fasthold\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"text\"][0])\n",
    "print(df_train[\"text\"][3])\n",
    "print(df_train[\"text\"][6])\n",
    "print(df_train[\"text\"][37])\n",
    "print(df_train[\"text\"][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a065c91",
   "metadata": {},
   "source": [
    "Now that we have handled the text to a sufficient degree, it is time to apply our machine learning models on the training data. This requires first a few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc378cc",
   "metadata": {},
   "source": [
    "First we need to convert our training data set into a numerical representation using a vectorizer. We can use the CountVectorizer, which uses the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971ececb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_data = vectorizer.fit_transform(df_train[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcee0d",
   "metadata": {},
   "source": [
    "We also need to take our test data and transform it so that the model can use the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fefb0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = vectorizer.transform(df_test[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0f4b",
   "metadata": {},
   "source": [
    "Next after setting up and transforming our data into a numerical representation, we need to choose which classifier or model we want to use on our data. With these sentiment analysis tasks, there are a couple popular models, such as the Naive Bayes Classifiers, Bernoulli and Multinomial. We first use the Bernoulli model to see the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9a329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB() accuracy: 0.6395334253104227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BernbClassifier = BernoulliNB()\n",
    "\n",
    "BernbClassifier.fit(X_train_data, df_train[\"label\"])\n",
    "prediction = BernbClassifier.predict(X_test_data)\n",
    "accuracy = accuracy_score(df_test[\"label\"], prediction)\n",
    "print(f\"{BernbClassifier} accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162731b",
   "metadata": {},
   "source": [
    "The accuracy of this model is about 64%. This means that using the Bernoulli Naive Bayes model, it has predicted 64% of the texts correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e85dd",
   "metadata": {},
   "source": [
    "We can try the Multinomial Naive Bayes model to see if it performs with a higher accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55677add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() accuracy: 0.727580584472595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MnbClassifier = MultinomialNB()\n",
    "\n",
    "MnbClassifier.fit(X_train_data, df_train[\"label\"])\n",
    "prediction = MnbClassifier.predict(X_test_data)\n",
    "accuracy = accuracy_score(df_test[\"label\"], prediction)\n",
    "print(f\"{MnbClassifier} accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746039e",
   "metadata": {},
   "source": [
    "Why does this model perform better than the previous one? As explained before, the multinomial model takes the number of occurrences of each word based on the sentiment to calculate the probability, to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ee063",
   "metadata": {},
   "source": [
    "With these classifiers, we left out an optional alpha value parameter, which is then chosen by default. The alpha value decides the smoothing of the estimated probabilities of the model. But how do we know which alpha value to choose so that it maximizes our performance? Cross validation should be the answer! Cross validation will use the model to check multiple values of alpha to determine the best alpha value to use as a parameter for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54ab231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alpha_vals = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "cv = GridSearchCV(MultinomialNB(),{'alpha': alpha_vals},cv=5,n_jobs=-1,verbose=0)\n",
    "\n",
    "cv.fit(X_train_data, df_train[\"label\"])\n",
    "print(\"Best alpha value:\", cv.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65154249",
   "metadata": {},
   "source": [
    "Given the 'best' alpha value, 10, we can now use this alpha value in our MultinomialNB classifier to see the new accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17715af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=10) accuracy: 0.6229775492286467\n"
     ]
    }
   ],
   "source": [
    "MnbClassifier = MultinomialNB(alpha=10)\n",
    "\n",
    "MnbClassifier.fit(X_train_data, df_train[\"label\"])\n",
    "prediction = MnbClassifier.predict(X_test_data)\n",
    "accuracy = accuracy_score(df_test[\"label\"], prediction)\n",
    "print(f\"{MnbClassifier} accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d4196",
   "metadata": {},
   "source": [
    "As we can see, with the alpha value of 10, the model performs worse. We have tried changing the range of the alpha_vals as well as the number of folds and nothing performs better than fine-tuning the different values, ourselves. We have found that with an alpha value of 0.2, the model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ec8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.2) accuracy: 0.7817634516493165\n"
     ]
    }
   ],
   "source": [
    "MnbClassifier = MultinomialNB(alpha=0.2)\n",
    "\n",
    "MnbClassifier.fit(X_train_data, df_train[\"label\"])\n",
    "prediction = MnbClassifier.predict(X_test_data)\n",
    "accuracy = accuracy_score(df_test[\"label\"], prediction)\n",
    "print(f\"{MnbClassifier} accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adeabd5",
   "metadata": {},
   "source": [
    "This is definitely a better result than using BernoulliNB and MultinomialNB (with a default alpha value), with an accuracy score of 78%. We determined that this result is acceptable. So in conclusion, we have taken our data, removed unnecessary characters and numbers, lemmatized the words into their ground forms, and then removed all the stop words. This turns our data into something more usable for the models we have chosen. The best result we have seen thus far is with a fine-tuned Multinomial Naive Bayes model, with a self-defined alpha value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3765eb5",
   "metadata": {},
   "source": [
    "# Task 3 Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c5f1b",
   "metadata": {},
   "source": [
    "In this task, the assignment is about training a convolutional neural network (CNN) as a binary classifier from the dataset that we have been provided with. This is a CIFAR-10 dataset that consists of 60000 images that are 32x32 colored images and will identify one of the following categories; airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. Each of these has 6000 images that are divided into 50000 for the training model and 10000 for the testing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "556554b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-76a39796b482>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
