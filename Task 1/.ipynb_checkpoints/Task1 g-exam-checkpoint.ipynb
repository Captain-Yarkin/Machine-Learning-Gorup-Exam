{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Machine learning on mushroom dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dython'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35988/3246915128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnominal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massociations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnominal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dython'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from dython.nominal import associations\n",
    "import matplotlib.pyplot as plt\n",
    "from dython import nominal\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_columns = 500\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all of our necessary libraries, sklearn for predicitive data analysis, classification, regression, clustering and more. Pandas for data analysis and data manipulation and so forth. We use numpy for numerical computing, this provides tools for working with arrays and matrices and other numerical data. We import KneighborsClassifier, LogsiticRegression and MLPClassifier as those are the machine learning models we will use in this task. Dython we use for work on nominal data. Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('agaricus-lepiota.csv', delimiter=',', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load read our data and turn it into a data frame, denoted by \"df\". A two-dimensional labeled data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for any duplicates that might skew the machines ability to learn\n",
    "duplicates = df.duplicated().sum()\n",
    "count = df.shape[0]\n",
    "print(f'{duplicates} duplicate rows in {count} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check for any duplicates in our dataframe (\"df\"), this is an important step in cleaning and preprocessing before applying machine learning models. This is due to the fact that duplicates can inflate performance by overfitting. However we read that there are 0 duplicate rows in our 8124 rows, therefore we dont need to get rid of any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = [\n",
    "    'class', 'cap shape', 'cap surface', 'cap color', 'bruised', 'odor',\n",
    "    'gill attachment', 'gill spacing', 'gill size', 'gill color', \n",
    "    'stalk shape', 'stalk root', 'stalk surface above ring',\n",
    "    'stalk surface below ring', 'stalk color above ring',\n",
    "    'stalk color below ring', 'veil type', 'veil color', 'ring number',\n",
    "    'ring type', 'spore print color', 'population', 'habitat'\n",
    "]\n",
    "\n",
    "df.columns = column_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is done to replace the numbers in the top row to something more descriptive, here we use the classifications from the \"agaricus-lepiota\" text file. We can now see how the \"1\" have turned into \"class\", and the other numbers have turned into other descriptive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].replace(['e', 'p'], [0, 1], inplace=True)\n",
    "df['stalk root'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is replacing the values in 'class'. It replaces 'e' (edible) with 0 and 'p' (poisonous) with 1. We change them from categorical to numerical.  \n",
    "We see that there are alot of the mushrooms that have \"stalk root\" type \"?\", a whooping 30.5% of the mushrooms have \"?\" as its stalk root feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['veil type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the veil type as well, and find that all the mushrooms have the same veil type. We therefor decide to drop it, as it wont impact performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.drop(['stalk root'])\n",
    "df.columns.drop(['veil type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theil_data = [[nominal.theils_u(df[\"class\"], df[col]) for col in df]]\n",
    "plt.figure(figsize=(20,1))\n",
    "sns.heatmap(theil_data, annot=True, fmt='.2f', xticklabels=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is calculating the Theil's U statistic between the target variable (the 'class' column) and each feature column in the DataFrame.\n",
    "\n",
    "The Theil's U statistic is a measure of the strength of association between two categorical variables. It ranges from 0 to 1, where 0 indicates no association and 1 indicates a strong association. The Theil's U statistic is useful for identifying which features are most informative for predicting the target variable.\n",
    "The code is using a list comprehension to calculate the Theil's U statistic for each column in the DataFrame using the theils_u() function from the dython.nominal module.\n",
    "\n",
    "The code then creates a heatmap visualization of the Theil's U statistic using the sns.heatmap() function from the Seaborn library. The heatmap is annotated with the Theil's U values for each feature and target pair.\n",
    "\n",
    "From this heatmap we can deduce that odor has a very close correlation to the precense of poison in the mushroom. Now we can take a closer look at odor to see how it correlates with the presence of poison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "ax=sns.countplot(x='odor',hue='class',data=df)\n",
    "for p in ax.patches:\n",
    "    patch_height = p.get_height()\n",
    "    if np.isnan(patch_height):\n",
    "        patch_height = 0    \n",
    "    ax.annotate('{}'.format(int(patch_height)), (p.get_x()+0.05, patch_height+10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we see that the presence of \"odor f\", meaning foul odor guarantees the presence of poison, there are a multitude of other smells that also guarantees poison. However \"odor n\", meaning no odor is weighted towards non-poisonous, however there are 120 instances of mushrooms that smell nothing that also are poisonous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns.drop([\"class\"]):\n",
    "    dummies = pd.get_dummies(df[column], prefix=column)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop([column], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is performing one-hot encoding on each categorical feature column in the DataFrame.\n",
    "\n",
    "For each column, it creates a new DataFrame dummies using the pd.get_dummies() function, which converts the column into a set of binary columns (one for each unique value in the original column), where a 1 indicates that the corresponding value is present and a 0 indicates that it is not.\n",
    "\n",
    "This process is done so that we convert categorical features into a numerical format, here we avoid assigning an arbitrary ordering to the categorical values that might not be meaningful for the model. And as there is no natural ordering for things like 'cap shape' or colors one-hot is well suited for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the effects of one-hot, we have split all the categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[\"class\"]\n",
    "data = df.drop(\"class\", axis=1)\n",
    "print(target.shape)\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=90)\n",
    "knn_scores = cross_val_score(knn, data, target, cv=5)  # 5-fold cross-validation\n",
    "knn_mean_score = np.mean(knn_scores)\n",
    "\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "rmse = sqrt(mse)\n",
    "print(f\"KNeighbor: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_scores = cross_val_score(log_model, data, target, cv=5)  # 5-fold cross-validation\n",
    "log_mean_score = np.mean(log_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = MLPClassifier(hidden_layer_sizes=(128, 128))\n",
    "nn_scores = cross_val_score(nn_model, data, target, cv=5)  # 5-fold cross-validation\n",
    "nn_mean_score = np.mean(nn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn_model.predict(x_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred, y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"KNeighbor (CV): {knn_mean_score}\")\n",
    "print(f\"Logistic Regression (CV): {log_mean_score}\")\n",
    "print(f\"Neural Network (CV): {nn_mean_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "\n",
    "corr = pd.DataFrame(corr[\"class\"])\n",
    "corr.sort_values(by=['class'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
